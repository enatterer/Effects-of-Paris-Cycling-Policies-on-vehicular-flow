{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import osmnx as ox\n",
    "import momepy\n",
    "from shapely.geometry import Polygon\n",
    "import alphashape\n",
    "from pyproj import Proj, Geod\n",
    "import ast\n",
    "from shapely.ops import unary_union\n",
    "from functions import get_exterior_coords\n",
    "\n",
    "data_path = '../../data/'  \n",
    "polygon_road_network = gpd.read_file(data_path + 'network/QGIS_Project/referentiel-comptages-edit.shp')\n",
    "paris_districts = gpd.read_file(data_path + 'districts_paris.geojson')\n",
    "df_car_detectors = gpd.read_file(data_path + 'all_car_detectors.geojson')\n",
    "\n",
    "paris_districts = gpd.read_file('../../data/districts_paris.geojson')\n",
    "polygon_all_districts = paris_districts.unary_union\n",
    "\n",
    "def get_polygon_geometry(df, start_point, end_point):\n",
    "    filtered_gdf = df[(df[\"c_ar\"] >= start_point) & (df[\"c_ar\"] <= end_point)]\n",
    "\n",
    "    # Check if there are any polygons matching the condition\n",
    "    if not filtered_gdf.empty:\n",
    "        # Apply unary_union to combine the selected polygons into a single polygon\n",
    "        districts_polygon = unary_union(filtered_gdf[\"geometry\"])\n",
    "    else:\n",
    "        # If no polygons match the condition, union_polygon will be None\n",
    "        districts_polygon = None\n",
    "\n",
    "    return districts_polygon\n",
    "\n",
    "def transform_highway(value):\n",
    "    if isinstance(value, list):\n",
    "        return value[0] if value else None\n",
    "    else:\n",
    "        return value\n",
    "    \n",
    "def filter_for_district(x_coords, y_coords, df):\n",
    "    district_polygon = Polygon(zip(x_coords, y_coords))\n",
    "\n",
    "    # Create a GeoDataFrame containing the district polygon\n",
    "    district_gdf = gpd.GeoDataFrame(geometry=[district_polygon], crs=df.crs)\n",
    "\n",
    "    # Use the GeoDataFrame's cx attribute to spatially filter cycleways_2010_2022\n",
    "    return gpd.overlay(df, district_gdf, how='intersection')\n",
    "\n",
    "\n",
    "def is_na_list(lst):\n",
    "    return lst is None or len(lst) == 0 or all(pd.isna(x) for x in lst)\n",
    "\n",
    "def parse_and_average_lanes(lanes_str):\n",
    "    if isinstance(lanes_str, list):\n",
    "        if is_na_list(lanes_str):\n",
    "            return np.nan\n",
    "        else: \n",
    "            return sum(map(int, lanes_str)) / len(lanes_str)\n",
    "    else:\n",
    "        if pd.isna(lanes_str):  # Check if input is NaN\n",
    "            return np.nan  # Return NaN if input is NaN\n",
    "    try:\n",
    "        # Attempt to parse the string as a list\n",
    "        lanes_list = ast.literal_eval(lanes_str)\n",
    "        if isinstance(lanes_list, list):\n",
    "            # If it's a list, calculate the average of list elements\n",
    "            return sum(map(int, lanes_list)) / len(lanes_list)\n",
    "        else:\n",
    "            # If it's a single integer, return it as is\n",
    "            return int(lanes_list)\n",
    "    except (SyntaxError, ValueError):\n",
    "        # If parsing fails or the lanes_str is not a list, parse as single integer\n",
    "        return int(lanes_str)\n",
    "\n",
    "def line_length_in_meters(line_string):\n",
    "    # Define a UTM projection for the zone containing your coordinates\n",
    "    utm_zone = 31  # Assuming you are in Paris, which falls in UTM zone 31 for example\n",
    "    proj = Proj(proj='utm', zone=utm_zone, ellps='WGS84')\n",
    "\n",
    "    # Extract coordinates from the LineString\n",
    "    coordinates = list(line_string.coords)\n",
    "\n",
    "    # Transform the coordinates to UTM projection\n",
    "    utm_coordinates = [proj(lon, lat) for lon, lat in coordinates]\n",
    "\n",
    "    # Compute the distance between consecutive points in meters\n",
    "    total_length = 0\n",
    "    geod = Geod(ellps='WGS84')\n",
    "    for i in range(len(utm_coordinates) - 1):\n",
    "        lon1, lat1 = utm_coordinates[i]\n",
    "        lon2, lat2 = utm_coordinates[i + 1]\n",
    "        distance_meters = geod.inv(lon1, lat1, lon2, lat2)[-1]  # Use [-1] to get distance\n",
    "\n",
    "        # Handle case of very small distances\n",
    "        if np.isnan(distance_meters):\n",
    "            dx = lon2 - lon1\n",
    "            dy = lat2 - lat1\n",
    "            distance_meters = np.sqrt(dx**2 + dy**2)\n",
    "        total_length += distance_meters\n",
    "\n",
    "    return total_length\n",
    "\n",
    "def map_highway(df):\n",
    "    highway_mapped = []\n",
    "    for value in df['highway']:\n",
    "        if isinstance(value, str):\n",
    "            highway_mapped.append(value)\n",
    "        elif isinstance(value, list):\n",
    "            highway_mapped.append(value[0] if len(value) > 0 else None)\n",
    "        else:\n",
    "            highway_mapped.append(None)\n",
    "    my_df = df.copy()\n",
    "    my_df['highway_mapped'] = highway_mapped\n",
    "    return my_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "The goal of this notebook is to investigate the OSM (historical) network of Paris, for different districts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_district_1_4, y_district_1_4  = get_exterior_coords(paris_districts, 1, 4)\n",
    "x_district_5_7, y_district_5_7  = get_exterior_coords(paris_districts, 5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m_/fjnjc1sn0ggc7z_2y7n27xfh0000gn/T/ipykernel_27079/1480800201.py:6: UserWarning: Approach is not set. Defaulting to 'primal'.\n",
      "  nodes, edges_2024 = momepy.nx_to_gdf(G_2024, points=True, lines=True)\n"
     ]
    }
   ],
   "source": [
    "alpha_shape = alphashape.alphashape(polygon_road_network, 435)\n",
    "coordinates = list(alpha_shape.exterior[0].coords)\n",
    "polygon = Polygon(coordinates)\n",
    "\n",
    "G_2024 = ox.graph_from_polygon(polygon=polygon, simplify=True, network_type=\"drive\")\n",
    "nodes, edges_2024 = momepy.nx_to_gdf(G_2024, points=True, lines=True)\n",
    "\n",
    "edges_2024['lanes_mapped'] = edges_2024['lanes'].apply(parse_and_average_lanes)\n",
    "edges_2024 = map_highway(edges_2024)\n",
    "\n",
    "average_lanes_per_highway_2024 = edges_2024.groupby('highway_mapped')['lanes_mapped'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set manually values for seldom highway classifications, which don't occur in the districts.\n",
    "\n",
    "average_lanes_per_highway_2024['road'] = 2\n",
    "average_lanes_per_highway_2024['virtual'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "highway_mapped\n",
       "living_street     1.188679\n",
       "primary           3.071071\n",
       "primary_link      1.842105\n",
       "residential       1.438535\n",
       "secondary         2.477971\n",
       "secondary_link    1.636364\n",
       "tertiary          2.073860\n",
       "tertiary_link     1.285714\n",
       "trunk             3.666667\n",
       "trunk_link        1.780000\n",
       "unclassified      1.683486\n",
       "road              2.000000\n",
       "virtual           1.000000\n",
       "Name: lanes_mapped, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_lanes_per_highway_2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m_/fjnjc1sn0ggc7z_2y7n27xfh0000gn/T/ipykernel_27079/1712476554.py:24: UserWarning: Approach is not set. Defaulting to 'primal'.\n",
      "  nodes, edges = momepy.nx_to_gdf(G, points=True, lines=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Year:  2023 , zone:  1\n",
      "Length in km: 84.25\n",
      "Length in lane km: 169.99334976941216\n",
      "Length of higher order roads in km: 28.75\n",
      "Length of higher order roads in lane km: 83.15877582088862\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m ox\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39moverpass_settings \u001b[39m=\u001b[39m overpass_settings\n\u001b[1;32m     21\u001b[0m ox\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39mlog_console \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m G \u001b[39m=\u001b[39m ox\u001b[39m.\u001b[39;49mgraph_from_polygon(polygon\u001b[39m=\u001b[39;49mdistrict, simplify\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, network_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdrive\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     24\u001b[0m nodes, edges \u001b[39m=\u001b[39m momepy\u001b[39m.\u001b[39mnx_to_gdf(G, points\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, lines\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mlanes\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m edges\u001b[39m.\u001b[39mcolumns:\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/osmnx/graph.py:432\u001b[0m, in \u001b[0;36mgraph_from_polygon\u001b[0;34m(polygon, network_type, simplify, retain_all, truncate_by_edge, clean_periphery, custom_filter)\u001b[0m\n\u001b[1;32m    429\u001b[0m poly_buff, _ \u001b[39m=\u001b[39m projection\u001b[39m.\u001b[39mproject_geometry(poly_proj_buff, crs\u001b[39m=\u001b[39mcrs_utm, to_latlong\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    431\u001b[0m \u001b[39m# download the network data from OSM within buffered polygon\u001b[39;00m\n\u001b[0;32m--> 432\u001b[0m response_jsons \u001b[39m=\u001b[39m downloader\u001b[39m.\u001b[39;49m_osm_network_download(poly_buff, network_type, custom_filter)\n\u001b[1;32m    434\u001b[0m \u001b[39m# create buffered graph from the downloaded data\u001b[39;00m\n\u001b[1;32m    435\u001b[0m bidirectional \u001b[39m=\u001b[39m network_type \u001b[39min\u001b[39;00m settings\u001b[39m.\u001b[39mbidirectional_network_types\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/osmnx/downloader.py:559\u001b[0m, in \u001b[0;36m_osm_network_download\u001b[0;34m(polygon, network_type, custom_filter)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[39mfor\u001b[39;00m polygon_coord_str \u001b[39min\u001b[39;00m polygon_coord_strs:\n\u001b[1;32m    558\u001b[0m     query_str \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00moverpass_settings\u001b[39m}\u001b[39;00m\u001b[39m;(way\u001b[39m\u001b[39m{\u001b[39;00mosm_filter\u001b[39m}\u001b[39;00m\u001b[39m(poly:\u001b[39m\u001b[39m{\u001b[39;00mpolygon_coord_str\u001b[39m!r}\u001b[39;00m\u001b[39m);>;);out;\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 559\u001b[0m     response_json \u001b[39m=\u001b[39m overpass_request(data\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: query_str})\n\u001b[1;32m    560\u001b[0m     response_jsons\u001b[39m.\u001b[39mappend(response_json)\n\u001b[1;32m    561\u001b[0m utils\u001b[39m.\u001b[39mlog(\n\u001b[1;32m    562\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot all network data within polygon from API in \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(polygon_coord_strs)\u001b[39m}\u001b[39;00m\u001b[39m request(s)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/osmnx/downloader.py:784\u001b[0m, in \u001b[0;36moverpass_request\u001b[0;34m(data, pause, error_pause)\u001b[0m\n\u001b[1;32m    782\u001b[0m utils\u001b[39m.\u001b[39mlog(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPost \u001b[39m\u001b[39m{\u001b[39;00mprepared_url\u001b[39m}\u001b[39;00m\u001b[39m with timeout=\u001b[39m\u001b[39m{\u001b[39;00msettings\u001b[39m.\u001b[39mtimeout\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    783\u001b[0m headers \u001b[39m=\u001b[39m _get_http_headers()\n\u001b[0;32m--> 784\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mpost(\n\u001b[1;32m    785\u001b[0m     url, data\u001b[39m=\u001b[39;49mdata, timeout\u001b[39m=\u001b[39;49msettings\u001b[39m.\u001b[39;49mtimeout, headers\u001b[39m=\u001b[39;49mheaders, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msettings\u001b[39m.\u001b[39;49mrequests_kwargs\n\u001b[1;32m    786\u001b[0m )\n\u001b[1;32m    787\u001b[0m sc \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus_code\n\u001b[1;32m    789\u001b[0m \u001b[39m# log the response size and domain\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(url, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, data\u001b[39m=\u001b[39;49mdata, json\u001b[39m=\u001b[39;49mjson, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/requests/sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n\u001b[0;32m--> 747\u001b[0m     r\u001b[39m.\u001b[39;49mcontent\n\u001b[1;32m    749\u001b[0m \u001b[39mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/requests/models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter_content(CONTENT_CHUNK_SIZE)) \u001b[39mor\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    901\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content_consumed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[39m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[39m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/urllib3/response.py:937\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \u001b[39mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[39m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[39m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 937\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_chunked(amt, decode_content\u001b[39m=\u001b[39mdecode_content)\n\u001b[1;32m    938\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    939\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/urllib3/response.py:1080\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1079\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1080\u001b[0m chunk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_chunk(amt)\n\u001b[1;32m   1081\u001b[0m decoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode(\n\u001b[1;32m   1082\u001b[0m     chunk, decode_content\u001b[39m=\u001b[39mdecode_content, flush_decoder\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m )\n\u001b[1;32m   1084\u001b[0m \u001b[39mif\u001b[39;00m decoded:\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/urllib3/response.py:1032\u001b[0m, in \u001b[0;36mHTTPResponse._handle_chunk\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# amt > self.chunk_left\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m     returned_chunk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39m_safe_read(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left)  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m-> 1032\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49m_safe_read(\u001b[39m2\u001b[39;49m)  \u001b[39m# type: ignore[union-attr] # Toss the CRLF at the end of the chunk.\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[39mreturn\u001b[39;00m returned_chunk\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/http/client.py:631\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_safe_read\u001b[39m(\u001b[39mself\u001b[39m, amt):\n\u001b[1;32m    625\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \n\u001b[1;32m    627\u001b[0m \u001b[39m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[39m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[1;32m    632\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m<\u001b[39m amt:\n\u001b[1;32m    633\u001b[0m         \u001b[39mraise\u001b[39;00m IncompleteRead(data, amt\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(data))\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "years = [2023, 2024]\n",
    "zones = [1, 2]\n",
    "\n",
    "def get_length_in_lane_km(df):\n",
    "    length_in_lane_km = 0\n",
    "    for idx, edge in df.iterrows():\n",
    "        length = edge['length_computed']\n",
    "        lanes = edge['lanes_mapped']\n",
    "        length_edge = length * lanes/1000\n",
    "        length_in_lane_km += length_edge\n",
    "    return length_in_lane_km\n",
    "\n",
    "for year in years:\n",
    "    for zone in zones:\n",
    "        if zone == 1:\n",
    "            district = get_polygon_geometry(paris_districts, 1, 4)\n",
    "        else: \n",
    "            district = get_polygon_geometry(paris_districts, 5, 7)\n",
    "        overpass_settings = '[out:json][timeout:90][date:\"' + str(year) + '-01-01T00:00:00Z\"]'\n",
    "        ox.settings.overpass_settings = overpass_settings\n",
    "        ox.settings.log_console = True\n",
    "    \n",
    "        G = ox.graph_from_polygon(polygon=district, simplify=True, network_type=\"drive\")\n",
    "        nodes, edges = momepy.nx_to_gdf(G, points=True, lines=True)\n",
    "        \n",
    "        if 'lanes' not in edges.columns:\n",
    "            edges['lanes'] = float('nan')\n",
    "        edges['lanes_mapped'] = edges['lanes'].apply(parse_and_average_lanes)\n",
    "        edges = map_highway(edges)  \n",
    "   \n",
    "        average_lanes_per_highway = edges.groupby('highway_mapped')['lanes_mapped'].mean()\n",
    "        edges = edges[edges['geometry'].notnull()]\n",
    "        edges['length_computed'] = edges['geometry'].apply(lambda x: line_length_in_meters(x))\n",
    "\n",
    "        for index, row in edges.iterrows():\n",
    "            if pd.isna(row['lanes_mapped']):\n",
    "                approximated_lanes = average_lanes_per_highway[row['highway_mapped']]\n",
    "                if (pd.isna(approximated_lanes)):\n",
    "                    approximated_lanes = average_lanes_per_highway_2024[row['highway_mapped']]\n",
    "                edges.at[index, 'lanes_mapped'] = approximated_lanes\n",
    "        \n",
    "        # filter for higher order roads\n",
    "        edges_hor = edges[\n",
    "            edges[\"highway\"].str.contains(\"motorway\") |\n",
    "            edges[\"highway\"].str.contains(\"trunk\") |\n",
    "            edges[\"highway\"].str.contains(\"primary\") |\n",
    "            edges[\"highway\"].str.contains(\"secondary\") |\n",
    "            edges[\"highway\"].str.contains(\"tertiary\") \n",
    "        ]\n",
    "    \n",
    "        length_in_km = edges['length_computed'].sum()/1000    \n",
    "        length_in_lane_km = get_length_in_lane_km(edges)\n",
    "        \n",
    "        length_hor_in_km = edges_hor['length_computed'].sum()/1000\n",
    "        length_in_lane_km_hor = get_length_in_lane_km(edges_hor)\n",
    "\n",
    "        print(\" \")\n",
    "        print(\"Year: \", year, \", zone: \", zone)\n",
    "        print(\"Length in km: \" + str(length_in_km.round(2)))\n",
    "        print(\"Length in lane km: \" + str(length_in_lane_km))\n",
    "        \n",
    "        print(\"Length of higher order roads in km: \" + str(length_hor_in_km.round(2)))\n",
    "        print(\"Length of higher order roads in lane km: \" + str(length_in_lane_km_hor))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paris_Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
