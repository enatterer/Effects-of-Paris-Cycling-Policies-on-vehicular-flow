{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.wkt import loads\n",
    "\n",
    "data_path = '../../data/' \n",
    "df_car_detectors = gpd.read_file(data_path + 'all_car_detectors.geojson')\n",
    "matched_detectors_2023 = pd.read_csv('../network_matching/output/detectors_matched_with_bvd_peripherique_2_osm_01_2024.csv', sep=\";\")\n",
    "\n",
    "def read_detector_data_2023():\n",
    "    ldd_2023 = pd.read_csv(data_path + '/traffic_data/traffic_data_2023.csv')\n",
    "    ldd_2023['t_1h'] = pd.to_datetime(ldd_2023['t_1h'])\n",
    "    return ldd_2023\n",
    "\n",
    "def get_merged_geodataframe(matched_detectors, ldd):\n",
    "    matched_detectors_without_dupl = matched_detectors.drop_duplicates(\n",
    "        subset='iu_ac', keep='first')\n",
    "    merged_ldd = pd.merge(ldd, matched_detectors_without_dupl[[\n",
    "                      'iu_ac', 'geometry_detector', 'highway', 'oneway', 'length_mapped_osm_street','score','length_detector_street','lanes_mapped']], on='iu_ac', how='inner')\n",
    "    merged_ldd['geometry_detector'] = merged_ldd['geometry_detector'].apply(loads)\n",
    "    return gpd.GeoDataFrame(merged_ldd, geometry='geometry_detector')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "The goal of this notebook is to find the optimal scaling parameter s. We try different values for s and compare it with the TomTom values for 5 and 6 am (\"free flow speed\"). In the last week the speeds were at 38 km/h at 5 am and 34 km/h at 6 am, for \"city centre\". One can expect that this will have been similar in 2023. Therefore, we can calibrate the speeds with the speeds from 2023. Note that we use the detector data for all detectors including those on the Boulevard Peripherique - one could \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldd_2023 = read_detector_data_2023()\n",
    "gdf_ldd_2023 = get_merged_geodataframe(matched_detectors_2023, ldd_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot matched_detectors\n",
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# matched_detectors_2023.plot(ax=ax, color='red', markersize=5)\n",
    "# matched_detectors_2023['geometry_detector'] = matched_detectors_2023['geometry_detector'].apply(loads)\n",
    "# gdf_detectors = gpd.GeoDataFrame(matched_detectors_2023, geometry='geometry_detector')\n",
    "\n",
    "# gdf_detectors.plot()\n",
    "\n",
    "# plot detectors with observed data\n",
    "# gdf_ldd_double = gdf_ldd_2023.copy()\n",
    "# gdf_ldd_double = gdf_ldd_double.drop_duplicates(subset='iu_ac', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_ldd = gdf_ldd_2023.copy()\n",
    "\n",
    "ldd_relevant = this_ldd.groupby([\"iu_ac\", \"day\"]).filter(\n",
    "    lambda x: len(x) == 18 and x[\"q\"].notnull().all() and x[\"k\"].notnull().all())\n",
    "ldd_relevant.reset_index(drop=True, inplace=True)\n",
    "\n",
    "grouped = ldd_relevant.groupby(\"t_1h\")\n",
    "grouped_5_6_7_am = [(hour, group) for hour, group in grouped if hour.hour in [5, 6, 7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "s: 0.005\n",
      "5 am mean: 35.01200321812179\n",
      "6 am mean: 31.059402726209143\n",
      "7 am mean: 26.66518901942787\n",
      " \n",
      "s: 0.0055\n",
      "5 am mean: 38.51320353993396\n",
      "6 am mean: 34.16534299883005\n",
      "7 am mean: 29.331707921370654\n",
      " \n",
      "s: 0.006\n",
      "5 am mean: 42.01440386174614\n",
      "6 am mean: 37.27128327145097\n",
      "7 am mean: 31.99822682331345\n",
      " \n",
      "s: 0.0065\n",
      "5 am mean: 45.515604183558324\n",
      "6 am mean: 40.377223544071875\n",
      "7 am mean: 34.66474572525623\n",
      " \n",
      "s: 0.007\n",
      "5 am mean: 49.01680450537051\n",
      "6 am mean: 43.483163816692795\n",
      "7 am mean: 37.33126462719902\n"
     ]
    }
   ],
   "source": [
    "s_values = [0.005, 0.0055, 0.006, 0.0065, 0.007]\n",
    "\n",
    "for s in s_values:\n",
    "    print(\" \")\n",
    "    print(\"s: \" +  str(s))\n",
    "    hour_2_q_per_lane_km = {}\n",
    "    hour_2_k_per_lane_km = {}\n",
    "    for hour, group in grouped_5_6_7_am:\n",
    "        length_street_segments = group['geometry_detector'].length.sum()\n",
    "        \n",
    "        q_per_lane_km_total = 0\n",
    "        k_per_lane_km_total = 0\n",
    "        for idx, row in group.iterrows():\n",
    "            q = row['q']\n",
    "            density = row['k']/s\n",
    "            length = row['geometry_detector'].length\n",
    "            lanes = row['lanes_mapped']\n",
    "            q_per_lane_km = (length * q) / lanes\n",
    "            k_per_lane_km = length * density \n",
    "            q_per_lane_km_total += q_per_lane_km\n",
    "            k_per_lane_km_total += k_per_lane_km\n",
    "        \n",
    "        q_hd = q_per_lane_km_total / length_street_segments\n",
    "        k_hd = k_per_lane_km_total / length_street_segments\n",
    "        hour_2_q_per_lane_km[hour] = q_hd\n",
    "        hour_2_k_per_lane_km[hour] = k_hd\n",
    "        \n",
    "    hours_5_am = []\n",
    "    hours_6_am = []\n",
    "    hours_7_am = []\n",
    "\n",
    "    for hour, q in hour_2_q_per_lane_km.items():\n",
    "        if hour.hour == 5:\n",
    "            v = q/hour_2_k_per_lane_km[hour]\n",
    "            hours_5_am.append(v * 100)\n",
    "        elif hour.hour == 6:\n",
    "            v = q/hour_2_k_per_lane_km[hour]\n",
    "            hours_6_am.append(v * 100)\n",
    "        elif hour.hour == 7:\n",
    "            v = q/hour_2_k_per_lane_km[hour]\n",
    "            hours_7_am.append(v * 100)\n",
    "            \n",
    "    hours_5_am = np.array(hours_5_am) \n",
    "    print(\"5 am mean: \" + str(hours_5_am.mean()))\n",
    "\n",
    "    hours_6_am = np.array(hours_6_am) \n",
    "    print(\"6 am mean: \" + str(hours_6_am.mean()))\n",
    "\n",
    "    hours_7_am = np.array(hours_7_am) \n",
    "    print(\"7 am mean: \" + str(hours_7_am.mean()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We find that s = 0.0055 fits best, thus we choose this as an optimal s. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paris_Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
