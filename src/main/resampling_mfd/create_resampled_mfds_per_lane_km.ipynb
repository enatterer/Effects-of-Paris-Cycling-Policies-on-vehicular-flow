{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "import geopandas as gpd\n",
    "import momepy\n",
    "from shapely.geometry import Polygon\n",
    "import alphashape\n",
    "import osmnx as ox\n",
    "from shapely.wkt import loads\n",
    "\n",
    "data_path = '../../data/' \n",
    "paris_districts = gpd.read_file(data_path + 'districts_paris.geojson')\n",
    "df_car_detectors = gpd.read_file(data_path + 'all_car_detectors.geojson')\n",
    "matched_detectors_2013 = pd.read_csv('../network_matching/output/detectors_matched_2_osm_01_2013.csv', sep=\";\")\n",
    "matched_detectors_2023 = pd.read_csv('../network_matching/output/detectors_matched_2_osm_01_2024.csv', sep=\";\")\n",
    "# best_matches_2013 = pd.read_csv('../network_matching/output/best_matches_2013.csv', sep=\";\")\n",
    "# best_matches_2022 = pd.read_csv('../network_matching/output/best_matches_2022.csv', sep=\";\")\n",
    "qgis_road_network = gpd.read_file(data_path + 'network/QGIS_Project/referentiel-comptages-edit.shp')\n",
    "alpha_shape = alphashape.alphashape(qgis_road_network, 435)\n",
    "coordinates = list(alpha_shape.exterior[0].coords)\n",
    "polygon = Polygon(coordinates)\n",
    "\n",
    "resultpath = 'results/'\n",
    "\n",
    "def merge_districts(districts_to_merge:list):\n",
    "    districts_to_merge = paris_districts[paris_districts['c_ar'].isin(districts_to_merge)]\n",
    "    merged_districts = districts_to_merge.unary_union\n",
    "    merged = gpd.GeoDataFrame(geometry=[merged_districts], crs=paris_districts.crs)\n",
    "    return merged\n",
    "\n",
    "def read_detector_data_2010():\n",
    "    ldd = pd.read_csv(data_path + '/traffic_data/traffic_data_2010_2012.csv')\n",
    "    # ldd_2013_2020 = pd.read_csv(data_path + 'traffic_data.csv')\n",
    "    # ldd_2021_2022 = pd.read_csv(data_path + '/traffic_data/traffic_data_2021_2022.csv')\n",
    "    # ldd = pd.concat([ldd_2010_2012, ldd_2021_2022])\n",
    "    ldd['t_1h'] = pd.to_datetime(ldd['t_1h'])\n",
    "    ldd_2010 = ldd[ldd['t_1h'].dt.year == 2010]\n",
    "    return ldd_2010\n",
    "\n",
    "def read_detector_data_2023():\n",
    "    ldd_2023 = pd.read_csv(data_path + '/traffic_data/traffic_data_2023.csv')\n",
    "    ldd_2023['t_1h'] = pd.to_datetime(ldd_2023['t_1h'])\n",
    "    return ldd_2023\n",
    "\n",
    "class ResampledMFD():\n",
    "    def __init__(self, ldd, p_sample: float, n_combinations: int):\n",
    "        self.ldd = ldd\n",
    "        self.p_sample = p_sample\n",
    "        self.n_combinations = n_combinations\n",
    "\n",
    "    def compute_resampled_mfd(self):\n",
    "        self.resampled_mfd = ResampledMFD.resample_mfd(\n",
    "            self.ldd, self.p_sample, self.n_combinations)\n",
    "        resampled_mfd_envelope, capacity, critical_occupancy = ResampledMFD.get_resampled_mfd_envelope(\n",
    "            self.resampled_mfd)\n",
    "        self.resampled_mfd_envelope = resampled_mfd_envelope\n",
    "        self.capacity = capacity\n",
    "        self.critical_occupancy = critical_occupancy\n",
    "        return\n",
    "\n",
    "    def print_resampled_mfd(self):\n",
    "        print(self.capacity, self.critical_occupancy)\n",
    "\n",
    "    def resample_mfd(ldd, p_sample, n_combinations):\n",
    "        n_population = ldd.iu_ac.nunique()\n",
    "        n_samples = int(n_population * p_sample)\n",
    "\n",
    "        population = ldd.iu_ac.unique().tolist()\n",
    "        population_subsets = []\n",
    "        seen_subsets = set()\n",
    "\n",
    "        while len(population_subsets) < n_combinations:\n",
    "            subsets_indices = tuple(\n",
    "                sorted(sample_without_replacement(n_population, n_samples)))\n",
    "            if subsets_indices not in seen_subsets:\n",
    "                subset = [population[n] for n in subsets_indices]\n",
    "                population_subsets.append(subset)\n",
    "                seen_subsets.add(subsets_indices)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        subsets_mfds = []\n",
    "        for idx, subset in enumerate(population_subsets):\n",
    "            # print(idx/len(population_subsets))\n",
    "            subset_ldd = ldd.loc[ldd.iu_ac.isin(subset)]\n",
    "\n",
    "            mfd = []\n",
    "            for tsp, temp in subset_ldd.groupby('t_1h'):\n",
    "                time = (tsp - datetime.datetime.combine(tsp.date(),\n",
    "                        datetime.time.min)).total_seconds()\n",
    "                flow = temp.q_per_lane_km.mean()\n",
    "                occupancy = temp.k_per_lane_km.mean()\n",
    "                # TODO Hier normalisieren mit den Anzahl an Lanes und der Länge der Straße.\n",
    "                mfd.append((tsp, time, flow, occupancy))\n",
    "            mfd = pd.DataFrame(\n",
    "                mfd, columns=['tsp', 'time', 'flow', 'occupancy'])\n",
    "            subsets_mfds.append(mfd)\n",
    "\n",
    "        resampled_mfd = pd.concat(subsets_mfds)\n",
    "        return resampled_mfd\n",
    "\n",
    "\n",
    "    def get_resampled_mfd_envelope(resampled_mfd):\n",
    "        # choose the number of bins that best fits occupancy values\n",
    "        resampled_mfd['occupancy_bin'] = pd.cut(resampled_mfd['occupancy'],\n",
    "                                                bins=int(resampled_mfd['occupancy'].max()))\n",
    "        # taking the median of top M flow values per occupancy bin\n",
    "        resampled_mfd_envelope = []\n",
    "        for bin, temp in resampled_mfd.groupby('occupancy_bin', observed=True):\n",
    "            upper_flow = temp.nlargest(50, 'flow', 'all').flow.median()\n",
    "            occupancy = bin.mid\n",
    "            resampled_mfd_envelope.append((upper_flow, occupancy))\n",
    "        resampled_mfd_envelope = pd.DataFrame(\n",
    "            resampled_mfd_envelope, columns=['flow', 'occupancy'])\n",
    "\n",
    "        # calculate the 95th/ 97.5th percentile of flow as the capacity\n",
    "        capacity = np.percentile(\n",
    "            resampled_mfd_envelope.flow, 95, method='nearest')\n",
    "\n",
    "        rounded_capacity = round(capacity, 2)\n",
    "\n",
    "        matching_rows = resampled_mfd_envelope.loc[round(\n",
    "            resampled_mfd_envelope.flow, 2) == rounded_capacity]\n",
    "        if not matching_rows.empty:\n",
    "            critical_occupancy = matching_rows['occupancy'].iloc[0]\n",
    "        else:\n",
    "            # Handle the case where no rows match the condition\n",
    "            # You might want to set a default value or raise an exception\n",
    "            critical_occupancy = None  # or any other suitable value\n",
    "\n",
    "        return resampled_mfd_envelope, capacity, critical_occupancy\n",
    "\n",
    "\n",
    "def get_ldd_for_district(district_list: list, gdf_ldd: gpd.GeoDataFrame):\n",
    "    districts = merge_districts(district_list)\n",
    "    ldd_within_districts = gpd.sjoin(\n",
    "        gdf_ldd, districts, how=\"inner\", op=\"within\")\n",
    "    ldd_within_districts.drop(columns=['index_right'], inplace=True)\n",
    "    ldd_within_districts = ldd_within_districts.groupby([\"iu_ac\", \"day\"]).filter(\n",
    "        lambda x: len(x) == 18 and x[\"q\"].notnull().all() and x[\"k\"].notnull().all())\n",
    "    ldd_within_districts.reset_index(drop=True, inplace=True)\n",
    "    return ldd_within_districts\n",
    "\n",
    "\n",
    "def filter_outliers(df, column_name):\n",
    "    # Calculate the first and third quartiles\n",
    "    Q1 = df[column_name].quantile(0.25)\n",
    "    Q3 = df[column_name].quantile(0.75)\n",
    "\n",
    "    # Calculate the IQR\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define the lower and upper bounds for outliers\n",
    "    lower_bound = Q1 - 8 * IQR\n",
    "    upper_bound = Q3 + 8 * IQR\n",
    "\n",
    "    # Filter the data\n",
    "    filtered_df = df[(df[column_name] >= lower_bound) &\n",
    "                     (df[column_name] <= upper_bound)]\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def is_parallel(linestring1, linestring2):\n",
    "    x1, y1 = linestring1.xy\n",
    "    x2, y2 = linestring2.xy\n",
    "    if abs(x2[-1] - x2[0]) < 1e-6 or abs(x1[-1] - x1[0]) < 1e-6:\n",
    "        return True\n",
    "    slope1 = (y1[-1] - y1[0]) / (x1[-1] - x1[0])\n",
    "    slope2 = (y2[-1] - y2[0]) / (x2[-1] - x2[0])\n",
    "    return abs(slope1 - slope2) < 0.2\n",
    "\n",
    "def get_road_network_graph(polygon):\n",
    "    ox.settings.log_console = True\n",
    "    G_road_network = ox.graph_from_polygon(\n",
    "        polygon, simplify=True, network_type=\"drive\")\n",
    "    nodes, edges = momepy.nx_to_gdf(G_road_network, points=True, lines=True)\n",
    "    edges['index'] = range(1, len(edges) + 1)\n",
    "    return nodes, edges\n",
    "\n",
    "def process_car_detectors(polygon):\n",
    "    df_car_detectors_without_multiples = df_car_detectors.drop_duplicates(\n",
    "        subset='iu_ac', keep='first')\n",
    "    boundary_gdf = gpd.GeoDataFrame(\n",
    "        geometry=[polygon], crs=df_car_detectors_without_multiples.crs)\n",
    "    car_detectors_within_boundary = gpd.sjoin(\n",
    "        df_car_detectors_without_multiples, boundary_gdf, op='within')\n",
    "    return car_detectors_within_boundary\n",
    "\n",
    "def get_merged_geodataframe(matched_detectors, ldd):\n",
    "    matched_detectors_without_dupl = matched_detectors.drop_duplicates(\n",
    "        subset='iu_ac', keep='first')\n",
    "    merged_ldd = pd.merge(ldd, matched_detectors_without_dupl[[\n",
    "                      'iu_ac', 'geometry_detector', 'highway', 'oneway', 'length_mapped_osm_street','score','length_detector_street','lanes_mapped']], on='iu_ac', how='inner')\n",
    "    merged_ldd['geometry_detector'] = merged_ldd['geometry_detector'].apply(loads)\n",
    "    return gpd.GeoDataFrame(merged_ldd, geometry='geometry_detector')\n",
    "\n",
    "def plot_mfds_for_district(district: str, mfd_2010: ResampledMFD, mfd_2022: ResampledMFD, p_sample, n_combinations):\n",
    "    fig, ax = plt.subplots()\n",
    "    # plt.gcf().set_size_inches(4, 3) \n",
    "    plt.scatter(mfd_2010.resampled_mfd['occupancy'],\n",
    "                mfd_2010.resampled_mfd['flow'], s=0.5, color='grey', label='resampled MFD 2010')\n",
    "    plt.hlines(y=mfd_2010.capacity, xmin=0, xmax=mfd_2010.critical_occupancy, color='orange',\n",
    "               linestyle='-')\n",
    "    plt.vlines(x=mfd_2010.critical_occupancy, ymin=0, ymax=mfd_2010.capacity, color='orange',\n",
    "               linestyle='-')\n",
    "    plt.scatter(mfd_2010.resampled_mfd_envelope['occupancy'], mfd_2010.resampled_mfd_envelope['flow'],\n",
    "                marker='s', s=10, color='orange', label='MFD envelope 2010')\n",
    "\n",
    "    plt.scatter(mfd_2022.resampled_mfd['occupancy'],\n",
    "                mfd_2022.resampled_mfd['flow'], s=0.5, color='darkgrey', label='resampled MFD 2023')\n",
    "    plt.hlines(y=mfd_2022.capacity, xmin=0, xmax=mfd_2022.critical_occupancy, color='blue',\n",
    "               linestyle='-')\n",
    "    plt.vlines(x=mfd_2022.critical_occupancy, ymin=0, ymax=mfd_2022.capacity, color='blue',\n",
    "               linestyle='-')\n",
    "    plt.scatter(mfd_2022.resampled_mfd_envelope['occupancy'], mfd_2022.resampled_mfd_envelope['flow'],\n",
    "                marker='s', s=10, color='blue', label='MFD envelope 2023')\n",
    "\n",
    "    # if district == str([1, 2, 3, 4]):\n",
    "    #     plt.xlim(0, 90)\n",
    "    #     plt.ylim(0, 7500)\n",
    "    # elif district == str([5, 6, 7]):\n",
    "    #     plt.xlim(0, 100)\n",
    "    #     plt.ylim(0, 7000)\n",
    "    plt.xlabel('Density [veh/lane-km]')\n",
    "    plt.ylabel('Flow [veh/lane-km/h]')\n",
    "    plt.title('Resampled MFD (lane km) for districts ' + district+ \"_\" + str(p_sample) + \"_\" + str(n_combinations) + ' in 2010 and 2023')\n",
    "   \n",
    "    plt.legend()\n",
    "    plt.savefig(resultpath + 'resampled_mfd_per_lane_km_district_' + district + \"_\" +\n",
    "                str(p_sample) + \"_\" + str(n_combinations) + \".pdf\", dpi=10, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elenanatterer/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3508: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "car_detectors = process_car_detectors(polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldd_2010 = read_detector_data_2010()\n",
    "ldd_2023 = read_detector_data_2023()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_ldd_2010 = get_merged_geodataframe(matched_detectors_2013, ldd_2010)\n",
    "gdf_ldd_2023 = get_merged_geodataframe(matched_detectors_2023, ldd_2023)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In gdf_merged_ldd sind weniger Einträge als in ldd. Es ist also anzunehmen, dass manche Detektoren, für die zwar gemessen wurde, nicht in dem eigentlichen Ding drin sind. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MFDs\n",
    "\n",
    "Zuerst erstellen wir die resampled MFDs. Zunächst für ldd_district_1_4_2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1, 2, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elenanatterer/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3508: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/var/folders/m_/fjnjc1sn0ggc7z_2y7n27xfh0000gn/T/ipykernel_97232/752475374.py:146: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\n",
      "Use `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n",
      "\n",
      "Left CRS: None\n",
      "Right CRS: EPSG:4326\n",
      "\n",
      "  ldd_within_districts = gpd.sjoin(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m idx, district \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(districts_to_test):\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(idx, district)\n\u001b[0;32m----> 6\u001b[0m     ldd_district_2010 \u001b[39m=\u001b[39m get_ldd_for_district(district, gdf_ldd_2010)\n\u001b[1;32m      7\u001b[0m     ldd_district_2023 \u001b[39m=\u001b[39m get_ldd_for_district(district, gdf_ldd_2023)\n\u001b[1;32m      9\u001b[0m     \u001b[39m# Map values to respective lane km\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 149\u001b[0m, in \u001b[0;36mget_ldd_for_district\u001b[0;34m(district_list, gdf_ldd)\u001b[0m\n\u001b[1;32m    146\u001b[0m ldd_within_districts \u001b[39m=\u001b[39m gpd\u001b[39m.\u001b[39msjoin(\n\u001b[1;32m    147\u001b[0m     gdf_ldd, districts, how\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minner\u001b[39m\u001b[39m\"\u001b[39m, op\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwithin\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    148\u001b[0m ldd_within_districts\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mindex_right\u001b[39m\u001b[39m'\u001b[39m], inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 149\u001b[0m ldd_within_districts \u001b[39m=\u001b[39m ldd_within_districts\u001b[39m.\u001b[39;49mgroupby([\u001b[39m\"\u001b[39;49m\u001b[39miu_ac\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mday\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39;49mfilter(\n\u001b[1;32m    150\u001b[0m     \u001b[39mlambda\u001b[39;49;00m x: \u001b[39mlen\u001b[39;49m(x) \u001b[39m==\u001b[39;49m \u001b[39m18\u001b[39;49m \u001b[39mand\u001b[39;49;00m x[\u001b[39m\"\u001b[39;49m\u001b[39mq\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mnotnull()\u001b[39m.\u001b[39;49mall() \u001b[39mand\u001b[39;49;00m x[\u001b[39m\"\u001b[39;49m\u001b[39mk\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mnotnull()\u001b[39m.\u001b[39;49mall())\n\u001b[1;32m    151\u001b[0m ldd_within_districts\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[39mreturn\u001b[39;00m ldd_within_districts\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/pandas/core/groupby/generic.py:1514\u001b[0m, in \u001b[0;36mDataFrameGroupBy.filter\u001b[0;34m(self, func, dropna, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[39mif\u001b[39;00m is_bool(res) \u001b[39mor\u001b[39;00m (is_scalar(res) \u001b[39mand\u001b[39;00m isna(res)):\n\u001b[1;32m   1513\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mand\u001b[39;00m notna(res):\n\u001b[0;32m-> 1514\u001b[0m         indices\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_index(name))\n\u001b[1;32m   1515\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1516\u001b[0m     \u001b[39m# non scalars aren't allowed\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   1518\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfilter function returned a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(res)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1519\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut expected a scalar bool\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1520\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:680\u001b[0m, in \u001b[0;36mBaseGroupBy._get_index\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_index\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[1;32m    677\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[39m    Safe get index, translate keys for datelike to underlying repr.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_indices([name])[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:666\u001b[0m, in \u001b[0;36mBaseGroupBy._get_indices\u001b[0;34m(self, names)\u001b[0m\n\u001b[1;32m    660\u001b[0m             msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    661\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmust supply a same-length tuple to get_group \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    662\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mwith multiple grouping keys\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    663\u001b[0m             )\n\u001b[1;32m    664\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m--> 666\u001b[0m     converters \u001b[39m=\u001b[39m [get_converter(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m index_sample]\n\u001b[1;32m    667\u001b[0m     names \u001b[39m=\u001b[39m (\u001b[39mtuple\u001b[39m(f(n) \u001b[39mfor\u001b[39;00m f, n \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(converters, name)) \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m names)\n\u001b[1;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:666\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    660\u001b[0m             msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    661\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmust supply a same-length tuple to get_group \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    662\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mwith multiple grouping keys\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    663\u001b[0m             )\n\u001b[1;32m    664\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m--> 666\u001b[0m     converters \u001b[39m=\u001b[39m [get_converter(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m index_sample]\n\u001b[1;32m    667\u001b[0m     names \u001b[39m=\u001b[39m (\u001b[39mtuple\u001b[39m(f(n) \u001b[39mfor\u001b[39;00m f, n \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(converters, name)) \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m names)\n\u001b[1;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:636\u001b[0m, in \u001b[0;36mBaseGroupBy._get_indices.<locals>.get_converter\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(s, datetime\u001b[39m.\u001b[39mdatetime):\n\u001b[1;32m    635\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlambda\u001b[39;00m key: Timestamp(key)\n\u001b[0;32m--> 636\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(s, np\u001b[39m.\u001b[39;49mdatetime64):\n\u001b[1;32m    637\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlambda\u001b[39;00m key: Timestamp(key)\u001b[39m.\u001b[39masm8\n\u001b[1;32m    638\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "districts_to_test = [[1, 2, 3, 4], [5,6,7]]\n",
    "\n",
    "for idx, district in enumerate(districts_to_test):\n",
    "    print(idx, district)\n",
    "    ldd_district_2010 = get_ldd_for_district(district, gdf_ldd_2010)\n",
    "    ldd_district_2023 = get_ldd_for_district(district, gdf_ldd_2023)\n",
    "\n",
    "    # Map values to respective lane km\n",
    "    ldd_district_2010['q_per_lane_km'] = ldd_district_2010['q'] * 1000 / (ldd_district_2010['length_detector_street'] * ldd_district_2010['lanes_mapped'])\n",
    "    ldd_district_2010['k_per_lane_km'] = ldd_district_2010['k'] * 1000 / (ldd_district_2010['length_detector_street'] * ldd_district_2010['lanes_mapped'])\n",
    "\n",
    "    ldd_district_2023['q_per_lane_km'] = ldd_district_2023['q'] * 1000 / (ldd_district_2023['length_detector_street'] * ldd_district_2023['lanes_mapped'])\n",
    "    ldd_district_2023['k_per_lane_km'] = ldd_district_2023['k'] * 1000 / (ldd_district_2023['length_detector_street'] * ldd_district_2023['lanes_mapped'])\n",
    "    ldd_district_2023['t_1h'] = ldd_district_2023['t_1h'].dt.tz_localize(None)\n",
    "    \n",
    "    # # Filter for outliers\n",
    "    # if district == [1, 2, 3, 4]:\n",
    "    #     ldd_district_2010 = ldd_district_2010[ldd_district_2010['k_per_lane_km'] < 180]\n",
    "    #     ldd_district_2022 = ldd_district_2023[ldd_district_2023['k_per_lane_km'] < 180]\n",
    "    # elif district == [5, 6, 7]:\n",
    "    #     ldd_district_2010 = ldd_district_2010[ldd_district_2010['k_per_lane_km'] < 100]\n",
    "    #     ldd_district_2022 = ldd_district_2022[ldd_district_2022['k_per_lane_km'] < 100]\n",
    "    # else:\n",
    "    #     print(\"NOT OK\")\n",
    "\n",
    "    resampled_district_2010 = ResampledMFD(ldd_district_2010, 0.6, 300)\n",
    "    resampled_district_2010.compute_resampled_mfd()\n",
    "    resampled_district_2010.print_resampled_mfd()\n",
    "\n",
    "    resampled_district_2023 = ResampledMFD(ldd_district_2023, 0.6, 300)\n",
    "    resampled_district_2023.compute_resampled_mfd()\n",
    "    resampled_district_2023.print_resampled_mfd()\n",
    "    \n",
    "    # resampled_mfd_object is your ResampledMFD object\n",
    "    with open('output/resampled_mfd_' + str(district) + '_2010_q95.pkl', 'wb') as f:\n",
    "        pickle.dump(resampled_district_2010, f)\n",
    "\n",
    "    with open('output/resampled_mfd_' + str(district) + '_2023_q95.pkl', 'wb') as f:\n",
    "        pickle.dump(resampled_district_2023, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paris_Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
